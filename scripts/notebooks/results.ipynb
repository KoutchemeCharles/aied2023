{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the best configs \n",
    "from src.utils.files import json2data\n",
    "from src.utils.code import clean_code\n",
    "from src.utils.distance import seq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_success_rate(dataframe, group=\"\"):\n",
    "    \"\"\" \n",
    "    Compute the ratio of number of buggy submissions which\n",
    "    were successfully repaired. \n",
    "    \n",
    "    Uses non empty repairs\n",
    "    \n",
    "    \"\"\"\n",
    "    f = lambda subdf: (subdf.repair != \"\").sum() / len(subdf)\n",
    "    if group:\n",
    "        return dataframe.groupby(group).apply(f)\n",
    "    return f(dataframe)\n",
    "\n",
    "def compute_seq_distance(dataframe, group=\"\"):\n",
    "    \"\"\" \n",
    "    Computes the average sequence edit distance between \n",
    "    the successfully repaired buggy programs and their\n",
    "    corrections. \n",
    "    \"\"\"\n",
    "    f = lambda subdf: subdf.loc[subdf.repair.astype(bool), \"seq_dist\"].mean()\n",
    "    if group:\n",
    "         return dataframe.groupby(group).apply(f)\n",
    "    return f(dataframe)\n",
    "\n",
    "\n",
    "def number_of_solutions(dataframe, group=\"\"):\n",
    "    \"\"\" \n",
    "    Compute the ratio of number of buggy submissions which\n",
    "    were successfully repaired. \n",
    "    \n",
    "    Uses non empty repairs\n",
    "    \n",
    "    \"\"\"\n",
    "    f = lambda subdf: (subdf.repair != \"\").sum()\n",
    "    if group:\n",
    "        return dataframe.groupby(group).apply(f)\n",
    "    return f(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(df, tool_name):\n",
    "    df[\"seq_dist\"] = [seq_dist(b, c) \n",
    "                      for b, c in df[[\"func_code\", \"repair\"]].to_numpy()]\n",
    "    \n",
    "    per_ass_sr = compute_success_rate(df, \"assignment_id\")\n",
    "    per_ass_sr = per_ass_sr.to_frame(f\"{tool_name}_SR\").reset_index()\n",
    "    per_ass_ds = compute_seq_distance(df, \"assignment_id\")\n",
    "    per_ass_ds = per_ass_ds.to_frame(f\"{tool_name}_SD\").reset_index(drop=True)\n",
    "    \n",
    "    print(\"Number of buggy programs\", len(df.repair))\n",
    "    print(\"Number of repairs found\", df.repair.astype(bool).sum())\n",
    "    print(\"Total success rate\", compute_success_rate(df), tool_name)\n",
    "    print(\"Total average distance\", compute_seq_distance(df), tool_name)\n",
    "    \n",
    "    #nb_sols = number_of_solutions(df, \"assignment_id\")\n",
    "    #nb_sols = nb_sols.to_frame(f\"{tool_name}_NB\")\n",
    "    descriptions = df.groupby('assignment_id', as_index=False).first().description\n",
    "    # nb_sols = df.groupby(\"assignment_id\", as_index=False).\n",
    "    agg = pd.concat([per_ass_sr, per_ass_ds, descriptions], axis=1)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenize_rt import src_to_tokens\n",
    "\n",
    "def doesnt_compiles(code):\n",
    "    try:\n",
    "        ast.parse(code)\n",
    "        src_to_tokens(code)\n",
    "        return False\n",
    "    except:\n",
    "        return True\n",
    "    \n",
    "def process_generic(df, ds_name, model_name):\n",
    "    print(\"Dataset\", ds_name)\n",
    "    print(\"Tool\", model_name)\n",
    "    df.loc[pd.isnull(df.repair), \"repair\"] = \"\"\n",
    "    df.loc[list(map(doesnt_compiles, df.func_code)), \"func_code\"] = \"\"\n",
    "    df.loc[list(map(doesnt_compiles, df.repair)), \"repair\"] = \"\"\n",
    "    df = df[df.func_code.astype(bool)]\n",
    "    df.repair = df.repair.apply(clean_code)\n",
    "    df.func_code = df.func_code.apply(clean_code)\n",
    "    df = get_results(df, model_name)\n",
    "    df['dataset'] = ds_name\n",
    "    \n",
    "    \n",
    "        \n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the results from Refactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_refactory_results(accepted_submission_ids=None):\n",
    "    # TODO: here it should be Refactory dublin_testing_results.csv \n",
    "    mapping = {\n",
    "        \"dublin\": \"./data/refactory/dublin_evaluation_results.csv\",\n",
    "        \"newcaledonia\": \"./data/refactory/newcaledonia_evaluation_results.csv\",\n",
    "        \"singapore\": \"./data/refactory/singapore_evaluation_results.csv\",\n",
    "    }\n",
    "    refactory_dataframe = []\n",
    "    for dataset_name, csv_path in mapping.items():\n",
    "        dataframe = pd.read_csv(csv_path)\n",
    "        \n",
    "        if accepted_submission_ids is not None:\n",
    "            asids = accepted_submission_ids[dataset_name]\n",
    "            dataframe = dataframe[dataframe.submission_id.isin(asids)]\n",
    "            \n",
    "        # Important: here we need to set to the empty string the repairs\n",
    "        # found by Refactory which did not pass all the tests\n",
    "        dataframe.loc[~dataframe.repair_correctness, \"repair\"] = \"\"\n",
    "        results = process_generic(dataframe, dataset_name, \"RF\")\n",
    "        refactory_dataframe.append(results)  \n",
    "        \n",
    "    refactory_dataframe = pd.concat(refactory_dataframe, axis=0, ignore_index=True)\n",
    "    return refactory_dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging with the other dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_results(model_dataframe, asids):\n",
    "    refactory_dataframe = load_refactory_results(asids)\n",
    "    print(\"rf\", refactory_dataframe)\n",
    "    refactory_dataframe.assignment_id = refactory_dataframe.assignment_id.astype(str)\n",
    "    tmp = refactory_dataframe.set_index([\"dataset\", \"assignment_id\"])\n",
    "    tmp = tmp[[c for c in tmp.columns if c not in model_dataframe]]\n",
    "    seq2seq_results = model_dataframe.join(tmp, on=[\"dataset\", \"assignment_id\"], rsuffix=\"r_\")\n",
    "    return seq2seq_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_assignments = [\n",
    "    \"bsearch\", \"factorial\", \"remove_zeros\", \n",
    "    \"swap_keys_values\", \"swap_unique_keys_values\", \"selection_sort\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the results from the Neural Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis: number of available correct solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_data = load_dataset(\"koutch/intro_prog\", \"dublin_data\")[\"train\"]\n",
    "train_data = train_data.to_pandas()\n",
    "# train_data = train_data[~train_data.assignment_id.isin(remove_assignments)]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of correct solutions in the trainnig set\", train_data.correct.sum())\n",
    "print(\"number of incorrect solutions in the training set\", len(train_data) - train_data.correct.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of solutions that Refactory managed to repair in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/refactory/training_results.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.loc[~df.repair_correctness, \"repair\"] = \"\"\n",
    "results = process_generic(df, \"training\", \"RF\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of programs repaired by refactory\", df.repair.astype(bool).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_repairs(df, details, dist_f):\n",
    "    repairs = []\n",
    "    for i, (buggy, predictions) in enumerate(df[[\"func_code\", \"generations\"]].to_numpy()):\n",
    "        repair, min_dist = \"\", np.inf \n",
    "        correct_predictions = []\n",
    "        for j, prediction in enumerate(predictions):\n",
    "            if details[str(i)][j][1][\"passed\"]:\n",
    "                if dist_f(buggy, prediction) < min_dist:\n",
    "                    repair = prediction\n",
    "        repairs.append(repair)\n",
    "        \n",
    "    df[\"repair\"] = repairs\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq2seq_results():\n",
    "    accepted_submission_ids = {}\n",
    "    results_folder = './data/seq2seq/results/'\n",
    "    datasets = [\"dublin\", \"newcaledonia\", \"singapore\"]\n",
    "    seq2seq_dataframe = []\n",
    "    for name in datasets:\n",
    "        path = os.path.join(results_folder, f\"{name}_evaluation_results.json\")\n",
    "        data = json2data(path)\n",
    "        model_dataframe = pd.DataFrame(data['eval_ds'])\n",
    "        model_dataframe = obtain_repairs(model_dataframe, data['details'], seq_dist)\n",
    "        accepted_submission_ids[name] = set(model_dataframe.submission_id)\n",
    "        print(\"dataset\", name, \"number of solutions to repair\", len(accepted_submission_ids[name]))\n",
    "        # TODO: average number of \n",
    "        model_dataframe = process_generic(model_dataframe, name, \"LLM\")\n",
    "        \n",
    "        seq2seq_dataframe.append(model_dataframe)\n",
    "        \n",
    "    seq2seq_dataframe = pd.concat(seq2seq_dataframe, axis=0, ignore_index=True)\n",
    "    return seq2seq_dataframe, accepted_submission_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq2seq_res, asids = get_seq2seq_results()\n",
    "results = merge_results(seq2seq_res, asids)\n",
    "columns = [\"dataset\", \"assignment_id\",# \"description\",\n",
    "           \"RF_SR\", \"LLM_SR\",\n",
    "           \"RF_SD\", \"LLM_SD\"]\n",
    "results = results[columns]\n",
    "results = results.round(2)\n",
    "# changing the original assignment ids in the Singapore dataset\n",
    "data_renaming = {\"dublin\": \"DB\", \n",
    "                 \"newcaledonia\": \"NC\", \"singapore\": \"SP\",\n",
    "                 \"1\": \"remove_extras\", \"3\": \"search\", \"4\": \"sort_age\", \"5\": \"top_k\"}\n",
    "results = results.replace(data_renaming)\n",
    "results = results.rename(columns={\"assignment_id\": \"assignment\"})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the results for Generative Models for Code Infilling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonlines(path):\n",
    "    with open(path, 'r') as fp:\n",
    "        return [json.loads(s) for s in fp.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_gmci_results():\n",
    "    mapping = {\n",
    "        'dublin': './data/gmci/results/dublin_evaluation_results.json',\n",
    "        'newcaledonia': './data/gmci/results/newcaledonia_evaluation_results.json',\n",
    "        'singapore': './data/gmci/results/singapore_evaluation_results.json',\n",
    "    }\n",
    "    \n",
    "    accepted_submission_ids = {}\n",
    "    gmci_dataframe = []\n",
    "    for dataset_name, csv_path in mapping.items():\n",
    "        dataframe = pd.DataFrame(read_jsonlines(csv_path))\n",
    "        accepted_submission_ids[dataset_name] = set(dataframe.submission_id)\n",
    "        dataframe = process_generic(dataframe, dataset_name, \"GMCI\")\n",
    "        dataframe['dataset'] = dataset_name\n",
    "        gmci_dataframe.append(dataframe)\n",
    "        \n",
    "    gmci_dataframe = pd.concat(gmci_dataframe, axis=0, ignore_index=True)\n",
    "    return gmci_dataframe, accepted_submission_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmci_res, asids = get_gmci_results()\n",
    "gmci_results = merge_results(gmci_res, asids)\n",
    "# reoordering the columns\n",
    "columns = [\"dataset\", \"assignment_id\",# \"description\",\n",
    "           \"RF_SR\", \"GMCI_SR\",\n",
    "           \"RF_SD\", \"GMCI_SD\"]\n",
    "gmci_results = gmci_results[columns]\n",
    "gmci_results = gmci_results.round(2)\n",
    "data_renaming = {\"dublin\": \"DB\", \n",
    "                 \"newcaledonia\": \"NC\", \"singapore\": \"SP\",\n",
    "                 \"1\": \"remove_extras\", \"3\": \"search\", \"4\": \"sort_age\", \"5\": \"top_k\"}\n",
    "gmci_results = gmci_results.replace(data_renaming)\n",
    "gmci_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gmci_results.to_latex(index=False, multicolumn=True, multirow=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the results from the GMCI model on Quixbugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/gmci/results/quixbugs_evaluation_results.json'\n",
    "dataframe = pd.DataFrame(read_jsonlines(path))\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of programs repaired\", dataframe.repair.astype(bool).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataframe))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (feedback)",
   "language": "python",
   "name": "feedback"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
